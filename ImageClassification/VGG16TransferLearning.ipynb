{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "141f4583-831b-427a-8e73-5ecfae1d7f6e",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69e9e0cf-4416-44a7-a863-3cbc7b793d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.transforms import v2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import warnings\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab68c3e-8452-4c21-a1ce-ee2a3fb6cebd",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb90c468-3d3f-4a50-990a-af3f0342bb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy function for binary classification\n",
    "def accuracy(pred, y):\n",
    "    pred = torch.sigmoid(pred)  # Convert logits to probabilities\n",
    "    pred_binary = (pred > 0.5).float()  # Threshold at 0.5\n",
    "    correct = pred_binary.eq(y).float().sum()\n",
    "    return (correct / y.size(0)).item()  # Return scalar accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8d37b23-3a87-4707-9e4f-ee20b098d077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(train_dataloader, val_dataloader, model, loss_fn, optimizer, n_epochs, logs_dict, device):\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        for batch_idx, (X, y) in enumerate(train_dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            pred = model.forward(X)\n",
    "            loss = loss_fn(pred.reshape(-1), y)\n",
    "            acc = accuracy(pred.reshape(-1), y)\n",
    "\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            sys.stdout.write(f'\\rEpoch: {epoch} Train loss: {loss:.3f} Train Accuracy: {acc:.2f} Val loss: {torch.mean(Tensor(LOGS[\"tmp_val_loss\"])).item():.3f} Val Accuracy: {torch.mean(Tensor(LOGS[\"tmp_val_acc\"])).item():.2f}')\n",
    "            sys.stdout.flush()\n",
    "            logs_dict[\"train_loss\"].append(loss.item())\n",
    "            logs_dict[\"train_acc\"].append(acc)\n",
    "        model.eval()\n",
    "        sys.stdout.write(f'\\rEpoch: {epoch} Validation in progress')\n",
    "        sys.stdout.flush()\n",
    "        for batch_idx, (X_val, y_val) in enumerate(val_dataloader):\n",
    "            with torch.no_grad():\n",
    "                X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "                val_pred = model.forward(X_val)\n",
    "                val_loss = loss_fn(val_pred.reshape(-1), y_val)\n",
    "                val_acc = accuracy(val_pred.reshape(-1), y_val)\n",
    "                logs_dict[\"tmp_val_loss\"].append(val_loss.item())\n",
    "                logs_dict[\"tmp_val_acc\"].append(val_acc)\n",
    "                logs_dict[\"val_loss\"].append(val_loss.item())\n",
    "                logs_dict[\"val_acc\"].append(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87ebee53-0b86-466b-86e9-7d68c73416b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SafeImageFolder(torchvision.datasets.ImageFolder): # This class its ImageFolder but with broken image filtering \n",
    "    def __init__(self, root, transform=None, target_transform=None):\n",
    "        super().__init__(root, transform=transform, target_transform=target_transform)\n",
    "        self.samples = [\n",
    "            (path, class_idx) for path, class_idx in self.samples\n",
    "            if self._is_valid_image(path)\n",
    "        ]\n",
    "\n",
    "    def _is_valid_image(self, path):\n",
    "        try:\n",
    "            with Image.open(path) as img:\n",
    "                img.verify()\n",
    "            return True\n",
    "        except Exception:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d69dde1-9d58-463e-95fc-8f46403cd9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGS = {\n",
    "    \"train_loss\": [], \n",
    "    \"train_acc\": [], \n",
    "    \"val_loss\": [], \n",
    "    \"val_acc\": [], \n",
    "    \"tmp_val_loss\": [], \n",
    "    \"tmp_val_acc\": [],\n",
    "    \"test_loss\": [],\n",
    "    \"test_acc\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db9f5e53-0407-43f7-a9b9-c16e5f63377c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    action='ignore',\n",
    "    category=UserWarning,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99200c99-b1f7-4895-b414-a285a122cc17",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "346586c0-f4a6-458c-8aed-df1cb65e9ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24fca7eb-6b92-4e8f-aebf-16d7f29916d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transforms = v2.Compose([v2.Resize((224, 224)), \n",
    "                        v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99afc53c-79a2-45b8-9751-d189fc20cde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SafeImageFolder(\"./catsvsdogs\", transform=image_transforms, target_transform=lambda x: torch.tensor(x, dtype=torch.float32))\n",
    "train_dataset, val_dataset = random_split(dataset, [0.8, 0.2], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9684185-c4e8-4749-8217-88360d63a054",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12bd47d7-2409-4627-81c1-5cc6cdfe6ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e3ce6a2-b01c-4559-9bd1-e020284c992b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.vgg16(weights=torchvision.models.VGG16_Weights.IMAGENET1K_V1)\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Linear(in_features=25088, out_features=512, bias=True),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Dropout(p=0.5, inplace=False),\n",
    "    nn.Linear(in_features=512, out_features=1, bias=True),\n",
    ")\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b7db44-1a56-432e-8069-25e7d6220c96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "123483e1-3016-424e-908c-33a76a64a0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 3e-4\n",
    "EPOCH = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8635414-a1a4-474e-b2fa-13974ce961e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2758f4c4-2a07-4cb8-b6b4-91e23806eed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train loss: 0.562 Train Accuracy: 0.81 Val loss: nan Val Accuracy: nan"
     ]
    }
   ],
   "source": [
    "train_fn(train_dataloader, val_dataloader, model, loss_fn, optimizer, EPOCH, LOGS, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b8c39b-4426-45f7-a015-41f5607374c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(12, 12))\n",
    "ax.flat[0].plot(LOGS[\"train_loss\"])\n",
    "ax.flat[0].set_title(\"Train Loss\")\n",
    "\n",
    "ax.flat[1].plot(LOGS[\"train_acc\"])\n",
    "ax.flat[1].set_title(\"Train Accuracy\")\n",
    "\n",
    "ax.flat[2].plot(LOGS[\"val_loss\"])\n",
    "ax.flat[2].set_title(\"Validation Loss\")\n",
    "\n",
    "ax.flat[3].plot(LOGS[\"val_acc\"])\n",
    "ax.flat[3].set_title(\"Validation Accuracy\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80c7917-6f06-441e-8bec-84875fbdb60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_model = model.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc7fab8-899f-4bf5-9397-3759fada908b",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af00e04-0895-40b7-a62c-9f7ea429f9cb",
   "metadata": {},
   "source": [
    "### Form Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8817c47b-c819-4f95-bb26-883692357dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_output = widgets.Output()\n",
    "\n",
    "image_id = widgets.BoundedIntText(\n",
    "    value=7,\n",
    "    min=0,\n",
    "    max=len(val_dataset),\n",
    "    step=1,\n",
    "    description='Image id from dataset:',\n",
    "    disabled=False,\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "def t1_func(b):\n",
    "    image_id_i = image_id.value\n",
    "    cat_or_dog = lambda x: \"Cat\" if torch.tanh(x).item() <= 0 else \"Dog\"  \n",
    "    with t1_output:\n",
    "        clear_output()\n",
    "        plt.figure(figsize=(12, 5))\n",
    "\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(val_dataset[image_id_i][0].permute(1, 2, 0).numpy())\n",
    "        plt.title(\"Selected image\")\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.axis('off')\n",
    "        prediction = cpu_model(val_dataset[image_id_i][0].unsqueeze(0))\n",
    "        info = f\"\"\"Raw Model Prediction: {prediction.item():.3f}\n",
    "\n",
    "Model Prediction in Words: {cat_or_dog(prediction)}\n",
    "Model Prediction Confidence: {torch.tanh(prediction).abs().item():.2%}\"\"\"\n",
    "\n",
    "        plt.text(0, 1, info, fontsize=12, va='top')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "t1_button = widgets.Button(description=\"Classify\")\n",
    "t1_button.on_click(t1_func)\n",
    "t1 = widgets.VBox([image_id, t1_button, t1_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31400fe-8bc9-491d-859f-721ed6bbbdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2_output = widgets.Output()\n",
    "\n",
    "image_file = widgets.FileUpload(\n",
    "    accept='image/*',\n",
    "    multiple=False,\n",
    "    desctiption=\"Image file Upload: \"\n",
    ")\n",
    "def t2_func(b):\n",
    "    filename = list(image_file.value.keys())[0]\n",
    "    bytes_content = image_file.value[filename]['content']\n",
    "    pil_image = Image.open(io.BytesIO(bytes_content))\n",
    "    cat_or_dog = lambda x: \"Cat\" if torch.tanh(x).item() <= 0 else \"Dog\" \n",
    "    with t2_output:\n",
    "        clear_output()\n",
    "        plt.figure(figsize=(12, 5))\n",
    "\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(pil_image)\n",
    "        plt.title(\"Selected image\")\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.axis('off')\n",
    "        prediction = cpu_model(image_transforms(pil_image).unsqueeze(0))\n",
    "        info = f\"\"\"Raw Model Prediction: {prediction.item():.3f}\n",
    "\n",
    "Model Prediction in Words: {cat_or_dog(prediction)}\n",
    "Model Prediction Confidence: {torch.tanh(prediction).abs().item():.2%}\"\"\"\n",
    "\n",
    "        plt.text(0, 1, info, fontsize=12, va='top')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "t2_button = widgets.Button(description=\"Classify\")\n",
    "t2_button.on_click(t2_func)\n",
    "\n",
    "t2 = widgets.VBox([image_file, t2_button, t2_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1e170f-64d7-43cb-b2d1-8569ecba1474",
   "metadata": {},
   "outputs": [],
   "source": [
    "form = widgets.Tab()\n",
    "form.children = [t1, t2]\n",
    "form.set_title(0, 'Select from dataset')\n",
    "form.set_title(1, 'Upload image file')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486967ad-572c-4cdb-a9f1-a7cd67e9afc2",
   "metadata": {},
   "source": [
    "### Prediction form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342eb878-bec2-44ab-9b50-617f519661ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c87590d-f080-4710-93a5-261c0f0118b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
