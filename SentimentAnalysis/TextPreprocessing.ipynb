{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfbe1aa0-cb9f-43b7-bd29-d9bb5d04a3be",
   "metadata": {},
   "source": [
    "### Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bacf59-3a82-43c6-a291-6498fe9f6c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "wget -P data/full_dataset/ https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_1.csv\n",
    "wget -P data/full_dataset/ https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_2.csv\n",
    "wget -P data/full_dataset/ https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_3.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a51fc49-2f65-4b08-9823-88eba9a7ab41",
   "metadata": {},
   "source": [
    "### Install Dependecies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "19894e29-5c8c-47ae-bbdc-f24b3b7b6c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emoji\n",
      "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
      "Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: emoji\n",
      "Successfully installed emoji-2.14.1\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15439ca9-66e1-4027-99c9-5f7392e5bc96",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "312e410b-392c-4b68-a688-83ad99d74a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import emoji as e\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27dc672-5a3c-4d55-97d1-6990817b12c2",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "049e32ff-ffcc-4ef9-b8a0-ec8806474d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = pd.read_csv(\"./data/full_dataset/goemotions_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "59bc366a-1ae4-4c24-931c-324ec5783ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML_CLEANER = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "URL_CLEANER = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7486cc76-c9f3-4429-aafa-41c3a061ec08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text): \n",
    "    punctuations = '''()-[]{};:,<>./@#$%^&*_~’'''\n",
    "    for x in text: \n",
    "        if x in punctuations: \n",
    "            text = text.replace(x, \"\") \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c15ab270-5e0f-4767-9383-34f9b9f73c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emoticons_to_text(text):\n",
    "    # Dictionary mapping common emoticons to text descriptions\n",
    "    emoticon_map = {\n",
    "        \":)\": \"Smile\",\n",
    "        \":‑)\": \"Smile\",\n",
    "        \":(\": \"Sad\",\n",
    "        \":‑(\": \"Sad\",\n",
    "        \":D\": \"Big grin\",\n",
    "        \":‑D\": \"Big grin\",\n",
    "        \";)\": \"Wink\",\n",
    "        \";‑)\": \"Wink\",\n",
    "        \":P\": \"Tongue out\",\n",
    "        \":‑P\": \"Tongue out\",\n",
    "        \":O\": \"Surprise\",\n",
    "        \":‑O\": \"Surprise\",\n",
    "        \":|\": \"Neutral\",\n",
    "        \":‑|\": \"Neutral\",\n",
    "        \":*\": \"Kiss\",\n",
    "        \":‑*\": \"Kiss\",\n",
    "        \":/\": \"Confused\",\n",
    "        \":‑/\": \"Confused\",\n",
    "        \">:(\": \"Angry\",\n",
    "        \">:‑(\": \"Angry\",\n",
    "        \"XD\": \"Laughing hard\",\n",
    "        \"x‑D\": \"Laughing hard\",\n",
    "        \":‑[\": \"Sad\",\n",
    "        \":‑]\": \"Happy\",\n",
    "        \":-{\": \"Sad\",\n",
    "        \":-}\": \"Happy\",\n",
    "        \":-@\": \"Screaming\",\n",
    "        \":-#\": \"Sealed lips\",\n",
    "        \":-X\": \"Sealed lips\",\n",
    "        \":-!\": \"Exclamation\",\n",
    "        \":-&\": \"Tongue-tied\",\n",
    "        \":-+\": \"Confused\",\n",
    "        \":-^\": \"Smirk\",\n",
    "        \":-<\": \"Sad\"\n",
    "    }\n",
    "    \n",
    "    # Replace emoticons with their text descriptions\n",
    "    for emoticon, description in emoticon_map.items():\n",
    "        text = text.replace(emoticon, description)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "eb2ab32a-7ee3-478c-a12e-b02b635a8429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_lemmatization(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.lemma_ for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ade8d988-1283-41ff-a81b-7fff9e0a281a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "    cleaned_text = text.lower() # Lower case text\n",
    "    cleaned_text = remove_punctuation(cleaned_text) # Remove some punctuation\n",
    "    cleaned_text = cleaned_text.strip() # Remove Whitespaces\n",
    "    cleaned_text = e.demojize(cleaned_text) # Convert emojis to text\n",
    "    cleaned_text = emoticons_to_text(cleaned_text)\n",
    "    cleaned_text = re.sub(HTML_CLEANER, '', cleaned_text) # Remove HTML\n",
    "    cleaned_text = re.sub(URL_CLEANER, '', cleaned_text) # Remove URLs\n",
    "    cleaned_text = text_lemmatization(cleaned_text) # Lemantization\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b9c9f6c1-3c2f-41c2-b2f4-d93adefa6757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That game hurt. -> that game hurt\n",
      "\n",
      " >sexuality shouldn’t be a grouping category It makes you different from othet ppl so imo it fits the definition of \"grouping\"  -> sexuality should not be a grouping category it make you different from othet ppl so imo it fit the definition of \" grouping \"\n",
      "\n",
      "You do right, if you don't care then fuck 'em! -> you do right if you do not care then fuck them !\n",
      "\n",
      "Man I love reddit. -> man I love reddit\n",
      "\n",
      "[NAME] was nowhere near them, he was by the Falcon.  -> name be nowhere near they he be by the falcon\n",
      "\n",
      "Right? Considering it’s such an important document, I should know the damned thing backwards and forwards... thanks again for the help! -> right ? consider its such an important document I should know the damned thing backwards and forwards thank again for the help !\n",
      "\n",
      "He isn't as big, but he's still quite popular. I've heard the same thing about his content. Never watched him much. -> he be not as big but he be still quite popular I have hear the same thing about his content never watch he much\n",
      "\n",
      "That's crazy; I went to a super [RELIGION] high school and I think I can remember 2 girls the entire 4 years that became teen moms. -> that be crazy I go to a super religion high school and I think I can remember 2 girl the entire 4 year that become teen mom\n",
      "\n",
      "that's adorable asf -> that be adorable asf\n",
      "\n",
      "\"Sponge Blurb Pubs Quaw Haha GURR ha AAa!\" finale is too real -> \" sponge blurb pub quaw haha gurr ha aaa ! \" finale be too real\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    text = dataset1.iloc[i].text\n",
    "    print(f\"{text} -> {text_preprocessing(text)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b30e2c0-c6e3-427c-8a50-95acd16da76f",
   "metadata": {},
   "source": [
    "To be continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
