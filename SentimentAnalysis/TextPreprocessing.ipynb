{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfbe1aa0-cb9f-43b7-bd29-d9bb5d04a3be",
   "metadata": {},
   "source": [
    "### Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bacf59-3a82-43c6-a291-6498fe9f6c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "wget -P data/full_dataset/ https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_1.csv\n",
    "wget -P data/full_dataset/ https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_2.csv\n",
    "wget -P data/full_dataset/ https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_3.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a51fc49-2f65-4b08-9823-88eba9a7ab41",
   "metadata": {},
   "source": [
    "### Install Dependecies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19894e29-5c8c-47ae-bbdc-f24b3b7b6c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in /home/ilya-prg/miniconda3/envs/main3.12/lib/python3.12/site-packages (2.14.1)\n",
      "Requirement already satisfied: symspellpy in /home/ilya-prg/miniconda3/envs/main3.12/lib/python3.12/site-packages (6.9.0)\n",
      "Requirement already satisfied: editdistpy>=0.1.3 in /home/ilya-prg/miniconda3/envs/main3.12/lib/python3.12/site-packages (from symspellpy) (0.1.6)\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji\n",
    "!pip install symspellpy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15439ca9-66e1-4027-99c9-5f7392e5bc96",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "312e410b-392c-4b68-a688-83ad99d74a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /home/ilya-prg/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import emoji as e\n",
    "import spacy\n",
    "import pkg_resources\n",
    "import requests\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "import nltk\n",
    "nltk.download('words')\n",
    "from nltk.corpus import words\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27dc672-5a3c-4d55-97d1-6990817b12c2",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f7ef9c9-0f97-4fe2-bb4d-a140f5730207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_spellcorrector():\n",
    "    sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "    dictionary_path = pkg_resources.resource_filename(\n",
    "      \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "    bigram_path = pkg_resources.resource_filename(\n",
    "      \"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\")\n",
    "    sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "    sym_spell.load_bigram_dictionary(bigram_path, term_index=0, count_index=2)\n",
    "    \n",
    "    return sym_spell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "049e32ff-ffcc-4ef9-b8a0-ec8806474d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = pd.read_csv(\"./data/full_dataset/goemotions_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "59bc366a-1ae4-4c24-931c-324ec5783ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML_CLEANER = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "URL_CLEANER = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "sym_spell = load_spellcorrector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e6f38497-b4d6-4141-9939-ff5ac7f4c148",
   "metadata": {},
   "outputs": [],
   "source": [
    "slang_meanings = {\n",
    "    \"brb\": \"Be right back\",\n",
    "    \"lmao\": \"Laughing my ass off\",\n",
    "    \"lol\": \"Laughing out loud\",\n",
    "    \"ppl\": \"People\",\n",
    "    \"afk\": \"Away from keyboard\",\n",
    "    \"asap\": \"As soon as possible\",\n",
    "    \"btw\": \"By the way\",\n",
    "    \"fyi\": \"For your information\",\n",
    "    \"gg\": \"Good game\",\n",
    "    \"gl\": \"Good luck\",\n",
    "    \"gr8\": \"Great\",\n",
    "    \"idk\": \"I don't know\",\n",
    "    \"ikr\": \"I know, right?\",\n",
    "    \"imo\": \"In my opinion\",\n",
    "    \"imho\": \"In my humble opinion\",\n",
    "    \"irl\": \"In real life\",\n",
    "    \"jk\": \"Just kidding\",\n",
    "    \"l8r\": \"Later\",\n",
    "    \"lmk\": \"Let me know\",\n",
    "    \"nvm\": \"Never mind\",\n",
    "    \"omw\": \"On my way\",\n",
    "    \"rofl\": \"Rolling on the floor laughing\",\n",
    "    \"smh\": \"Shaking my head\",\n",
    "    \"tbh\": \"To be honest\",\n",
    "    \"tldr\": \"Too long; didn't read\",\n",
    "    \"ttyl\": \"Talk to you later\",\n",
    "    \"w/e\": \"Whatever\",\n",
    "    \"w/o\": \"Without\",\n",
    "    \"wtf\": \"What the fuck\",\n",
    "    \"yolo\": \"You only live once\",\n",
    "    \"b4\": \"Before\",\n",
    "    \"cu\": \"See you\",\n",
    "    \"dm\": \"Direct message\",\n",
    "    \"fomo\": \"Fear of missing out\",\n",
    "    \"ftw\": \"For the win\",\n",
    "    \"gtg\": \"Got to go\",\n",
    "    \"hmu\": \"Hit me up\",\n",
    "    \"nsfw\": \"Not safe for work\",\n",
    "    \"op\": \"Original poster\",\n",
    "    \"srsly\": \"Seriously\",\n",
    "    \"tmi\": \"Too much information\",\n",
    "    \"u\": \"You\",\n",
    "    \"ur\": \"Your\",\n",
    "    \"yw\": \"You're welcome\"\n",
    "}\n",
    "\n",
    "emoticon_map = {\n",
    "        \":)\": \"Smile\",\n",
    "        \":‑)\": \"Smile\",\n",
    "        \":(\": \"Sad\",\n",
    "        \":‑(\": \"Sad\",\n",
    "        \":D\": \"Big grin\",\n",
    "        \":‑D\": \"Big grin\",\n",
    "        \";)\": \"Wink\",\n",
    "        \";‑)\": \"Wink\",\n",
    "        \":P\": \"Tongue out\",\n",
    "        \":‑P\": \"Tongue out\",\n",
    "        \":O\": \"Surprise\",\n",
    "        \":‑O\": \"Surprise\",\n",
    "        \":|\": \"Neutral\",\n",
    "        \":‑|\": \"Neutral\",\n",
    "        \":*\": \"Kiss\",\n",
    "        \":‑*\": \"Kiss\",\n",
    "        \":/\": \"Confused\",\n",
    "        \":‑/\": \"Confused\",\n",
    "        \">:(\": \"Angry\",\n",
    "        \">:‑(\": \"Angry\",\n",
    "        \"XD\": \"Laughing hard\",\n",
    "        \"x‑D\": \"Laughing hard\",\n",
    "        \":‑[\": \"Sad\",\n",
    "        \":‑]\": \"Happy\",\n",
    "        \":-{\": \"Sad\",\n",
    "        \":-}\": \"Happy\",\n",
    "        \":-@\": \"Screaming\",\n",
    "        \":-#\": \"Sealed lips\",\n",
    "        \":-X\": \"Sealed lips\",\n",
    "        \":-!\": \"Exclamation\",\n",
    "        \":-&\": \"Tongue-tied\",\n",
    "        \":-+\": \"Confused\",\n",
    "        \":-^\": \"Smirk\",\n",
    "        \":-<\": \"Sad\"\n",
    "    }\n",
    "correlation_whitelist = {\n",
    "    \"reddit\",\n",
    "    \"youtube\",\n",
    "    \"facebook\",\n",
    "    \"twitter\",\n",
    "    \"tiktok\",\n",
    "    \"snapchat\",\n",
    "    \"instagram\",\n",
    "    \"whatsapp\",\n",
    "    \"linkedin\",\n",
    "    \"\\\"\",\n",
    "    \"!\",\n",
    "    \"?\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7486cc76-c9f3-4429-aafa-41c3a061ec08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text): \n",
    "    punctuations = '''()-[]{};:,<>./@#$%^&*_~’'''\n",
    "    for x in text: \n",
    "        if x in punctuations: \n",
    "            text = text.replace(x, \"\") \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c15ab270-5e0f-4767-9383-34f9b9f73c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_words_with_dict(text, dictionary):\n",
    "    sorted_words = sorted(dictionary.keys(), key=len, reverse=True)\n",
    "    \n",
    "    for word in sorted_words:\n",
    "        pattern = r'\\b' + re.escape(word) + r'\\b'\n",
    "        text = re.sub(pattern, dictionary[word], text, flags=re.IGNORECASE)\n",
    "    \n",
    "    return text.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eb2ab32a-7ee3-478c-a12e-b02b635a8429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_lemmatization(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.lemma_ for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc8a8fc5-6c27-4636-a42a-d67fd02514bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_correction(text, sym_spell=sym_spell, whitelist=correlation_whitelist):\n",
    "    words = text.split()\n",
    "    corrected_words = []\n",
    "    for word in words:\n",
    "        if word in whitelist:\n",
    "            corrected_words.append(word)\n",
    "        else:\n",
    "            suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=2)\n",
    "            if suggestions:\n",
    "                best = suggestions[0].term\n",
    "                if best != word:\n",
    "                    corrected_words.append(best)\n",
    "                else:\n",
    "                    corrected_words.append(word)\n",
    "            else:\n",
    "                corrected_words.append(word)\n",
    "    return \" \".join(corrected_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b81a14e0-37ad-49b0-a6b0-de8f45a221e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_repeats_if_not_real_word(text, wordlist=set(words.words())):\n",
    "    tokens = text.split()\n",
    "    new_tokens = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        clean_token = re.sub(r'(.)\\1{2,}', r'\\1', token.lower())\n",
    "        \n",
    "        if token.lower() in wordlist:\n",
    "            new_tokens.append(token)\n",
    "        else:\n",
    "            new_tokens.append(clean_token)\n",
    "            \n",
    "    return \" \".join(new_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ade8d988-1283-41ff-a81b-7fff9e0a281a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "    cleaned_text = text.lower() # Lower case text\n",
    "    cleaned_text = replace_words_with_dict(cleaned_text, slang_meanings) # Convert slang to normal words\n",
    "    cleaned_text = re.sub(r'\\d+', '', cleaned_text) # Remove numbers\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z\\s]', '', cleaned_text) # keep only english words\n",
    "    cleaned_text = remove_punctuation(cleaned_text) # Remove some punctuation\n",
    "    cleaned_text = cleaned_text.strip() # Remove Whitespaces\n",
    "    cleaned_text = re.sub(HTML_CLEANER, '', cleaned_text) # Remove HTML\n",
    "    cleaned_text = re.sub(URL_CLEANER, '', cleaned_text) # Remove URLs\n",
    "    cleaned_text = e.demojize(cleaned_text) # Convert emojis to text\n",
    "    cleaned_text = replace_words_with_dict(cleaned_text, emoticon_map) # Convert emoticons to text\n",
    "    cleaned_text = remove_repeats_if_not_real_word(cleaned_text) # reduce repeating characters by 2\n",
    "    cleaned_text = text_correction(cleaned_text) # Autocorrect\n",
    "    cleaned_text = text_lemmatization(cleaned_text) # Lemantization\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b9c9f6c1-3c2f-41c2-b2f4-d93adefa6757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That game hurt. -> that game hurt\n",
      "\n",
      " >sexuality shouldn’t be a grouping category It makes you different from othet ppl so imo it fits the definition of \"grouping\"  -> sexuality should not be a grouping category it make you different from other people so in my opinion it fit the definition of group\n",
      "\n",
      "You do right, if you don't care then fuck 'em! -> you do right if you do not care then fuck pm\n",
      "\n",
      "Man I love reddit. -> man a love reddit\n",
      "\n",
      "[NAME] was nowhere near them, he was by the Falcon.  -> name be nowhere near they he be by the falcon\n",
      "\n",
      "Right? Considering it’s such an important document, I should know the damned thing backwards and forwards... thanks again for the help! -> right consider its such an important document a should know the damned thing backwards and forwards thank again for the help\n",
      "\n",
      "He isn't as big, but he's still quite popular. I've heard the same thing about his content. Never watched him much. -> he be not as big but he a still quite popular a be hear the same thing about his content never watch he much\n",
      "\n",
      "That's crazy; I went to a super [RELIGION] high school and I think I can remember 2 girls the entire 4 years that became teen moms. -> that a crazy a go to a super religion high school and a think a can remember girl the entire year that become teen mon\n",
      "\n",
      "that's adorable asf -> that a adorable as\n",
      "\n",
      "\"Sponge Blurb Pubs Quaw Haha GURR ha AAa!\" finale is too real -> sponge blurb pub quad hama guru a a finale be too real\n",
      "\n",
      "CPU times: user 28.9 ms, sys: 0 ns, total: 28.9 ms\n",
      "Wall time: 28.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(10):\n",
    "    text = dataset1.iloc[i].text\n",
    "    print(f\"{text} -> {text_preprocessing(text)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ab6ddd57-d98c-460f-9ba5-feea074a9c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 36s, sys: 18.7 ms, total: 2min 36s\n",
      "Wall time: 2min 37s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                                       that game hurt\n",
       "1    sexuality should not be a grouping category it...\n",
       "2         you do right if you do not care then fuck pm\n",
       "3                                    man a love reddit\n",
       "4        name be nowhere near they he be by the falcon\n",
       "Name: preprocessed_text, dtype: object"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "dataset1[\"preprocessed_text\"] = dataset1.text.apply(text_preprocessing)\n",
    "dataset1[\"preprocessed_text\"].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ac28a9-3c5f-47b9-bb0e-94acb77f71fa",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5df52445-7b8c-4cf5-b02c-19ec5b03c80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 17835\n",
      "['aah' 'aaron' 'aba' ... 'zookeeper' 'zoom' 'zugzwang']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(dataset1[\"preprocessed_text\"])\n",
    "\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "print(f\"Number of unique words: {len(vocab)}\")\n",
    "print(vocab)\n",
    "### my text preprocessing kinda not works :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f9e166ff-ab6d-43b3-9636-656bcf07e5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 5000\n",
      "['aba' 'abandon' 'ability' ... 'zombie' 'zone' 'zoom']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    min_df=5,\n",
    "    max_df=0.7,\n",
    ")\n",
    "X = vectorizer.fit_transform(dataset1[\"preprocessed_text\"])\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "print(f\"Number of unique words: {len(vocab)}\")\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a022a0-d62d-4079-b1bf-0f98772a7ba0",
   "metadata": {},
   "source": [
    "# Simple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4224bf45-5876-4fdd-851a-a61694cca6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple model to see difference between changes of text_processing functions\n",
    "y_columns = dataset1.columns[9:-1]\n",
    "y = dataset1[y_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b08f2ff0-1e18-4938-be1a-3744f08f1344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((70000, 5000), (70000, 28))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d7ca1d8d-4b19-4631-981e-4554ed74d15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy on Train Dataset: 0.12137142857142857\n"
     ]
    }
   ],
   "source": [
    "lr = OneVsRestClassifier(LogisticRegression(max_iter=50000, C=1.0, random_state=42), n_jobs=-1)\n",
    "scores = cross_val_score(lr, X, y, cv=5, scoring=\"accuracy\")\n",
    "lr.fit(X, y)\n",
    "print(f\"Model Accuracy on Train Dataset: {scores.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "00725518-861e-44ae-88a8-cf54f5c80336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy on Train Dataset: 0.12404285714285712\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(n_jobs=-1)\n",
    "scores = cross_val_score(xgb, X, y, cv=5, scoring=\"accuracy\")\n",
    "xgb.fit(X, y)\n",
    "print(f\"Model Accuracy on Train Dataset: {scores.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b30e2c0-c6e3-427c-8a50-95acd16da76f",
   "metadata": {},
   "source": [
    "To be continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
